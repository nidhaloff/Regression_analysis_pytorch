{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidhaloff/Regression_analysis_pytorch/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQxOnZNs_xc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20dfee7b-9601-44be-ff51-60cbcc82c539"
      },
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.autograd import Variable\n",
        "%matplotlib inline\n",
        "\n",
        "# run keras model on google GPU\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThvGDpEW_zvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Criate data for a y = 2x + 1 equation with noise\n",
        "x_train = np.arange(0,10,.1).reshape(-1,1)\n",
        "y_train = (2 * x_train + 1) + (np.random.randn(len(x_train))).reshape(-1,1)\n",
        "#Turn the numpy arrays to float32 to avoid an error when converting to tensors\n",
        "x_train = x_train.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTMlBYXd_2oo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearRegressionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "    \n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhbsxpgrC5Fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = x_train.shape[1]\n",
        "output_dim = 1\n",
        "#Create an instance of the NN\n",
        "model = LinearRegressionModel(input_dim, output_dim)\n",
        "#Define the loss function to MSE\n",
        "criterion = nn.MSELoss()\n",
        "#Learning rate and optimizer (SGD)\n",
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cNKDx0lDGNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).float())\n",
        "loader = DataLoader(trainset, batch_size=16, shuffle=True)\n",
        "def train(epochs=10):\n",
        "    train_losses, test_losses, accuracy_list, iters = [], [], [], []\n",
        "    total = 0\n",
        "    count = 0\n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        for x, y in loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # count += 1\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                x_test = torch.from_numpy(x_train).float()\n",
        "                y_test = torch.from_numpy(y_train).float()\n",
        "                pred = model(x_test)\n",
        "                loss = criterion(pred, y_test)\n",
        "                test_losses.append(loss.item())\n",
        "                score = r2_score(y_test.numpy(), pred.numpy())\n",
        "                print(\"score: \", score)\n",
        "                accuracy_list.append(score)\n",
        "\n",
        "        print(f\"epoch: {e} => train loss= {sum(train_losses)/len(train_losses)}, test loss= {sum(test_losses)/len(test_losses)}\")\n",
        "\n",
        "    return train_losses, test_losses, accuracy_list\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLhHExSZEHk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14843023-0a54-445e-85a6-9978b596b62d"
      },
      "source": [
        "train_losses, test_losses, accs = train(500)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score:  -7.5638174753468\n",
            "epoch: 0 => train loss= 313.97247314453125, test loss= 270.4411315917969\n",
            "score:  -6.772177751866901\n",
            "epoch: 1 => train loss= 291.8109239850725, test loss= 257.9413375854492\n",
            "score:  -6.028713243782492\n",
            "epoch: 2 => train loss= 278.3014410109747, test loss= 245.94865926106772\n",
            "score:  -5.381197236170038\n",
            "epoch: 3 => train loss= 263.4332754952567, test loss= 234.84026336669922\n",
            "score:  -4.819275429036005\n",
            "epoch: 4 => train loss= 248.6387721470424, test loss= 224.62618103027344\n",
            "score:  -4.267339612892993\n",
            "epoch: 5 => train loss= 238.41457294282458, test loss= 214.9118169148763\n",
            "score:  -3.815419394419834\n",
            "epoch: 6 => train loss= 226.10281216368384, test loss= 205.93421064104353\n",
            "score:  -3.381589227943765\n",
            "epoch: 7 => train loss= 216.24670927865165, test loss= 197.48848915100098\n",
            "score:  -2.9724993769966908\n",
            "epoch: 8 => train loss= 207.63646213592045, test loss= 189.48416561550565\n",
            "score:  -2.6219930774074416\n",
            "epoch: 9 => train loss= 198.751254599435, test loss= 181.97382507324218\n",
            "score:  -2.3159925639463794\n",
            "epoch: 10 => train loss= 190.14024982204685, test loss= 174.950515053489\n",
            "score:  -2.0142834205129114\n",
            "epoch: 11 => train loss= 182.79523490724108, test loss= 168.30377133687338\n",
            "score:  -1.7554457886232586\n",
            "epoch: 12 => train loss= 175.56625169188112, test loss= 162.05083700326773\n",
            "score:  -1.5184259058568896\n",
            "epoch: 13 => train loss= 168.79475223774813, test loss= 156.15653773716517\n",
            "score:  -1.3074007765180191\n",
            "epoch: 14 => train loss= 162.3552569798061, test loss= 150.60387420654297\n",
            "score:  -1.1168903482478787\n",
            "epoch: 15 => train loss= 156.26464867591858, test loss= 145.36927938461304\n",
            "score:  -0.9024898422305361\n",
            "epoch: 16 => train loss= 151.4148601243476, test loss= 140.35224488202263\n",
            "score:  -0.7350084000320443\n",
            "epoch: 17 => train loss= 146.1919634380038, test loss= 135.59882630242242\n",
            "score:  -0.5786469327050621\n",
            "epoch: 18 => train loss= 141.3343297054893, test loss= 131.08588248804995\n",
            "score:  -0.43759078154604225\n",
            "epoch: 19 => train loss= 136.6736630848476, test loss= 126.80150890350342\n",
            "score:  -0.3046589821213852\n",
            "epoch: 20 => train loss= 132.33413583405164, test loss= 122.7252698625837\n",
            "score:  -0.17780811972525967\n",
            "epoch: 21 => train loss= 128.3054581803161, test loss= 118.83751227638938\n",
            "score:  -0.06920162626481452\n",
            "epoch: 22 => train loss= 124.3686223000473, test loss= 115.1387013974397\n",
            "score:  0.025980536780664365\n",
            "epoch: 23 => train loss= 120.57526035535903, test loss= 111.62288284301758\n",
            "score:  0.10912144738288243\n",
            "epoch: 24 => train loss= 116.89558832441057, test loss= 108.28330780029297\n",
            "score:  0.1873637457610714\n",
            "epoch: 25 => train loss= 113.45997804075807, test loss= 105.10559030679556\n",
            "score:  0.2616067583911774\n",
            "epoch: 26 => train loss= 110.21918524888457, test loss= 102.07642385694716\n",
            "score:  0.3212658607842118\n",
            "epoch: 27 => train loss= 107.0418330747254, test loss= 99.1963404927935\n",
            "score:  0.3828927327899234\n",
            "epoch: 28 => train loss= 104.12726490133501, test loss= 96.4477750843969\n",
            "score:  0.4348960888484208\n",
            "epoch: 29 => train loss= 101.2581200327192, test loss= 93.82770601908366\n",
            "score:  0.4824811057268983\n",
            "epoch: 30 => train loss= 98.5295945883896, test loss= 91.32819914048717\n",
            "score:  0.5311090739835943\n",
            "epoch: 31 => train loss= 95.99000793482575, test loss= 88.93692243099213\n",
            "score:  0.5676136239008245\n",
            "epoch: 32 => train loss= 93.4806452943133, test loss= 86.65563840577097\n",
            "score:  0.6061485593567129\n",
            "epoch: 33 => train loss= 91.14622862499301, test loss= 84.47275593701531\n",
            "score:  0.6421300169151933\n",
            "epoch: 34 => train loss= 88.9163843281415, test loss= 82.38214457375663\n",
            "score:  0.6718500906778921\n",
            "epoch: 35 => train loss= 86.75750682751338, test loss= 80.38160758548312\n",
            "score:  0.6977593001398448\n",
            "epoch: 36 => train loss= 84.67393409607493, test loss= 78.46709421518686\n",
            "score:  0.7214880510875135\n",
            "epoch: 37 => train loss= 82.6800300241413, test loss= 76.63362515600105\n",
            "score:  0.743357076505138\n",
            "epoch: 38 => train loss= 80.76929026296287, test loss= 74.87647213079991\n",
            "score:  0.7633552665032902\n",
            "epoch: 39 => train loss= 78.94043986882482, test loss= 73.19138842821121\n",
            "score:  0.7806724607878254\n",
            "epoch: 40 => train loss= 77.1782910948431, test loss= 71.57516567881514\n",
            "score:  0.7976699179370766\n",
            "epoch: 41 => train loss= 75.4963063707157, test loss= 70.02312564849854\n",
            "score:  0.8120353225651268\n",
            "epoch: 42 => train loss= 73.87500325548292, test loss= 68.5327234711758\n",
            "score:  0.8265796975594997\n",
            "epoch: 43 => train loss= 72.33537054990794, test loss= 67.09962811253287\n",
            "score:  0.838566187681254\n",
            "epoch: 44 => train loss= 70.83826542316922, test loss= 65.721814166175\n",
            "score:  0.8515468029180359\n",
            "epoch: 45 => train loss= 69.41092307486149, test loss= 64.39499383387358\n",
            "score:  0.8620015195337757\n",
            "epoch: 46 => train loss= 68.02656403319814, test loss= 63.11760936899388\n",
            "score:  0.8720007898796757\n",
            "epoch: 47 => train loss= 66.69742331370001, test loss= 61.88687067230543\n",
            "score:  0.8807122550039634\n",
            "epoch: 48 => train loss= 65.41290145260947, test loss= 60.70075184958322\n",
            "score:  0.8889076856874398\n",
            "epoch: 49 => train loss= 64.18026172331402, test loss= 59.556901626586914\n",
            "score:  0.8963452209369138\n",
            "epoch: 50 => train loss= 62.988768532162624, test loss= 58.45330290700875\n",
            "score:  0.902717125527411\n",
            "epoch: 51 => train loss= 61.83679655882028, test loss= 57.388280643866615\n",
            "score:  0.9085648143411732\n",
            "epoch: 52 => train loss= 60.72230891686566, test loss= 56.35996361498563\n",
            "score:  0.9136328976650704\n",
            "epoch: 53 => train loss= 59.64659615294643, test loss= 55.36676855882009\n",
            "score:  0.9189654119449866\n",
            "epoch: 54 => train loss= 58.613697340581325, test loss= 54.40662790211764\n",
            "score:  0.9237576939181716\n",
            "epoch: 55 => train loss= 57.61433595114825, test loss= 53.478075521332876\n",
            "score:  0.9280611691303299\n",
            "epoch: 56 => train loss= 56.64429347228287, test loss= 52.57971968567162\n",
            "score:  0.9315922894070896\n",
            "epoch: 57 => train loss= 55.7038504936425, test loss= 51.710419034135754\n",
            "score:  0.935337209575106\n",
            "epoch: 58 => train loss= 54.798968510824025, test loss= 50.86858174760463\n",
            "score:  0.9381316490563979\n",
            "epoch: 59 => train loss= 53.91658731528691, test loss= 50.053334919611615\n",
            "score:  0.9409138685352009\n",
            "epoch: 60 => train loss= 53.06257736040781, test loss= 49.263377150551214\n",
            "score:  0.9433764524493736\n",
            "epoch: 61 => train loss= 52.233693685193764, test loss= 48.497647579639185\n",
            "score:  0.94561080952749\n",
            "epoch: 62 => train loss= 51.43075030548773, test loss= 47.75510688622793\n",
            "score:  0.9476780620425176\n",
            "epoch: 63 => train loss= 50.65130219867985, test loss= 47.0347505453974\n",
            "score:  0.949751085254112\n",
            "epoch: 64 => train loss= 49.89613400691158, test loss= 46.33555185978229\n",
            "score:  0.9516169791590114\n",
            "epoch: 65 => train loss= 49.16286707692074, test loss= 45.65664822585655\n",
            "score:  0.953257366683591\n",
            "epoch: 66 => train loss= 48.45255802568596, test loss= 44.9972371983884\n",
            "score:  0.9547101239063432\n",
            "epoch: 67 => train loss= 47.76367591377817, test loss= 44.35654594617731\n",
            "score:  0.9561205213557127\n",
            "epoch: 68 => train loss= 47.09294896090006, test loss= 43.73377995214601\n",
            "score:  0.957438002382528\n",
            "epoch: 69 => train loss= 46.440684297559216, test loss= 43.12821291003908\n",
            "score:  0.9586148329675428\n",
            "epoch: 70 => train loss= 45.805657937912876, test loss= 42.539180659912\n",
            "score:  0.9594821471687047\n",
            "epoch: 71 => train loss= 45.18593832785412, test loss= 41.96613000995583\n",
            "score:  0.9604259169361437\n",
            "epoch: 72 => train loss= 44.5835914708047, test loss= 41.40837110721902\n",
            "score:  0.9613065274322532\n",
            "epoch: 73 => train loss= 43.99685589439851, test loss= 40.86531096857947\n",
            "score:  0.9619914219709358\n",
            "epoch: 74 => train loss= 43.42529680865152, test loss= 40.33644405206044\n",
            "score:  0.962774838917869\n",
            "epoch: 75 => train loss= 42.86962515216573, test loss= 39.82116915991432\n",
            "score:  0.9633842550156152\n",
            "epoch: 76 => train loss= 42.330097215516226, test loss= 39.319028095765546\n",
            "score:  0.963931898856763\n",
            "epoch: 77 => train loss= 41.803351971910985, test loss= 38.82954072188108\n",
            "score:  0.9644616280044387\n",
            "epoch: 78 => train loss= 41.28934147754586, test loss= 38.352233678479735\n",
            "score:  0.9649628131430411\n",
            "epoch: 79 => train loss= 40.787258552121266, test loss= 37.886661471426486\n",
            "score:  0.9654049407134188\n",
            "epoch: 80 => train loss= 40.29634790803179, test loss= 37.432412502206404\n",
            "score:  0.9658006499340892\n",
            "epoch: 81 => train loss= 39.8179226087257, test loss= 36.989090381599055\n",
            "score:  0.9661910841220465\n",
            "epoch: 82 => train loss= 39.350851780576264, test loss= 36.55630217115563\n",
            "score:  0.96650772064138\n",
            "epoch: 83 => train loss= 38.894641039728306, test loss= 36.13369940434183\n",
            "score:  0.9668811063063115\n",
            "epoch: 84 => train loss= 38.451774831329075, test loss= 35.720901510294745\n",
            "score:  0.9671550155208692\n",
            "epoch: 85 => train loss= 38.01576635867753, test loss= 35.31760298651318\n",
            "score:  0.9674204745546106\n",
            "epoch: 86 => train loss= 37.58994603281832, test loss= 34.923479336431654\n",
            "score:  0.9676910068790257\n",
            "epoch: 87 => train loss= 37.174197641842355, test loss= 34.53821595825932\n",
            "score:  0.9679496379606845\n",
            "epoch: 88 => train loss= 36.767993103706814, test loss= 34.16151841570822\n",
            "score:  0.9681315094301799\n",
            "epoch: 89 => train loss= 36.370377606129836, test loss= 33.79312811427646\n",
            "score:  0.9683275191590333\n",
            "epoch: 90 => train loss= 35.98398985534766, test loss= 33.43276628164145\n",
            "score:  0.9685154071420282\n",
            "epoch: 91 => train loss= 35.604519007114334, test loss= 33.0801739083684\n",
            "score:  0.9686677619848046\n",
            "epoch: 92 => train loss= 35.23370672684569, test loss= 32.735112432510626\n",
            "score:  0.9687870295273803\n",
            "epoch: 93 => train loss= 34.868663620328284, test loss= 32.39735262191042\n",
            "score:  0.9689747524679562\n",
            "epoch: 94 => train loss= 34.51280031632212, test loss= 32.0666411424938\n",
            "score:  0.9690905702857071\n",
            "epoch: 95 => train loss= 34.16365094028325, test loss= 31.742781387021143\n",
            "score:  0.9692226967794829\n",
            "epoch: 96 => train loss= 33.82378766135983, test loss= 31.425556136775263\n",
            "score:  0.9693229942396963\n",
            "epoch: 97 => train loss= 33.48797831758205, test loss= 31.114772551521963\n",
            "score:  0.9693521509566432\n",
            "epoch: 98 => train loss= 33.159456085010525, test loss= 30.810258121803553\n",
            "score:  0.9694154884556759\n",
            "epoch: 99 => train loss= 32.83721219030874, test loss= 30.511813979148865\n",
            "score:  0.9694933770132874\n",
            "epoch: 100 => train loss= 32.52121280378213, test loss= 30.219255267983616\n",
            "score:  0.969529979410875\n",
            "epoch: 101 => train loss= 32.21120177804171, test loss= 29.932421669656154\n",
            "score:  0.9695963153991131\n",
            "epoch: 102 => train loss= 31.90713055159034, test loss= 29.65113731784728\n",
            "score:  0.9696165608263609\n",
            "epoch: 103 => train loss= 31.60934565856971, test loss= 29.375256132621033\n",
            "score:  0.969662201813179\n",
            "epoch: 104 => train loss= 31.317357405047026, test loss= 29.10461610044752\n",
            "score:  0.9696898089791777\n",
            "epoch: 105 => train loss= 31.030489459131765, test loss= 28.839074259096723\n",
            "score:  0.9697357401173707\n",
            "epoch: 106 => train loss= 30.74981314964463, test loss= 28.578482260770887\n",
            "score:  0.9697906790793387\n",
            "epoch: 107 => train loss= 30.4745641348578, test loss= 28.32269997563627\n",
            "score:  0.9698162741714623\n",
            "epoch: 108 => train loss= 30.202899571425167, test loss= 28.07160352840336\n",
            "score:  0.9698301025054852\n",
            "epoch: 109 => train loss= 29.936927469693995, test loss= 27.825068500908937\n",
            "score:  0.9698705817224008\n",
            "epoch: 110 => train loss= 29.675436416705605, test loss= 27.58296402993503\n",
            "score:  0.9698847176834927\n",
            "epoch: 111 => train loss= 29.418328226658, test loss= 27.345178867025034\n",
            "score:  0.9698977025525968\n",
            "epoch: 112 => train loss= 29.167366332392174, test loss= 27.111598662570515\n",
            "score:  0.9699012307644759\n",
            "epoch: 113 => train loss= 28.920041765877627, test loss= 26.882115378714445\n",
            "score:  0.9699222929319989\n",
            "epoch: 114 => train loss= 28.676598277391854, test loss= 26.656617324766906\n",
            "score:  0.9699317886278657\n",
            "epoch: 115 => train loss= 28.43721019720723, test loss= 26.435004583720502\n",
            "score:  0.9699443600026625\n",
            "epoch: 116 => train loss= 28.20163426254404, test loss= 26.217176701268578\n",
            "score:  0.9699666863378072\n",
            "epoch: 117 => train loss= 27.97078763316532, test loss= 26.003034841711237\n",
            "score:  0.9699803469580857\n",
            "epoch: 118 => train loss= 27.74336799288712, test loss= 25.792488379638737\n",
            "score:  0.9699817670514568\n",
            "epoch: 119 => train loss= 27.51968085616827, test loss= 25.585450651745003\n",
            "score:  0.9700071009674482\n",
            "epoch: 120 => train loss= 27.30047493033032, test loss= 25.381828423373957\n",
            "score:  0.970012719614743\n",
            "epoch: 121 => train loss= 27.08379775505574, test loss= 25.181542809869423\n",
            "score:  0.9700279508093188\n",
            "epoch: 122 => train loss= 26.870758344348992, test loss= 24.984509962361034\n",
            "score:  0.9700371706975897\n",
            "epoch: 123 => train loss= 26.66187937770571, test loss= 24.79065271587141\n",
            "score:  0.9700452371172483\n",
            "epoch: 124 => train loss= 26.45580680343083, test loss= 24.599895147323608\n",
            "score:  0.9700447827078632\n",
            "epoch: 125 => train loss= 26.254547018245233, test loss= 24.412165590694972\n",
            "score:  0.9700463367182733\n",
            "epoch: 126 => train loss= 26.05504262601565, test loss= 24.227392018779994\n",
            "score:  0.9700403713684803\n",
            "epoch: 127 => train loss= 25.860524896066636, test loss= 24.045507005415857\n",
            "score:  0.970039261387405\n",
            "epoch: 128 => train loss= 25.66704675043672, test loss= 23.86644218642582\n",
            "score:  0.9700413890536739\n",
            "epoch: 129 => train loss= 25.476286353973244, test loss= 23.690131694078445\n",
            "score:  0.970043104495641\n",
            "epoch: 130 => train loss= 25.288812228546536, test loss= 23.516512551380476\n",
            "score:  0.9700521005213136\n",
            "epoch: 131 => train loss= 25.104015895943622, test loss= 23.345521849664774\n",
            "score:  0.9700555034336118\n",
            "epoch: 132 => train loss= 24.92195499948477, test loss= 23.17710162867281\n",
            "score:  0.9700716739206934\n",
            "epoch: 133 => train loss= 24.742725418447687, test loss= 23.01119133162854\n",
            "score:  0.9700796619418376\n",
            "epoch: 134 => train loss= 24.566481034213272, test loss= 22.847737096415624\n",
            "score:  0.9700832862089389\n",
            "epoch: 135 => train loss= 24.39225993389968, test loss= 22.6866857584785\n",
            "score:  0.9700909810362999\n",
            "epoch: 136 => train loss= 24.222279823702742, test loss= 22.527983761181797\n",
            "score:  0.9700864943252938\n",
            "epoch: 137 => train loss= 24.05387089912072, test loss= 22.37158281949983\n",
            "score:  0.9700846920726809\n",
            "epoch: 138 => train loss= 23.888859921895957, test loss= 22.217432660593403\n",
            "score:  0.9700914292046477\n",
            "epoch: 139 => train loss= 23.72531862246747, test loss= 22.065483126895767\n",
            "score:  0.9700904446453914\n",
            "epoch: 140 => train loss= 23.563980061987372, test loss= 21.915689126396856\n",
            "score:  0.9700977474958231\n",
            "epoch: 141 => train loss= 23.404435061772823, test loss= 21.768003276536163\n",
            "score:  0.9700927562310805\n",
            "epoch: 142 => train loss= 23.24703799353375, test loss= 21.62238406509786\n",
            "score:  0.9701046907965677\n",
            "epoch: 143 => train loss= 23.092135157997884, test loss= 21.47878472548392\n",
            "score:  0.9701071506599367\n",
            "epoch: 144 => train loss= 22.938939072492676, test loss= 21.33716553079671\n",
            "score:  0.9701089448797938\n",
            "epoch: 145 => train loss= 22.787978981484873, test loss= 21.19748593683112\n",
            "score:  0.9701106182410091\n",
            "epoch: 146 => train loss= 22.63873506369987, test loss= 21.05970638625476\n",
            "score:  0.9701115664119894\n",
            "epoch: 147 => train loss= 22.492904721496764, test loss= 20.92378851892175\n",
            "score:  0.9701112421196572\n",
            "epoch: 148 => train loss= 22.348695742103878, test loss= 20.7896951212979\n",
            "score:  0.9701080650720815\n",
            "epoch: 149 => train loss= 22.205711376624446, test loss= 20.657390304406483\n",
            "score:  0.9701150417119582\n",
            "epoch: 150 => train loss= 22.06507326482091, test loss= 20.526836410263517\n",
            "score:  0.9701199713844804\n",
            "epoch: 151 => train loss= 21.925738088156383, test loss= 20.397999306258402\n",
            "score:  0.9701219644005544\n",
            "epoch: 152 => train loss= 21.788285310375812, test loss= 20.270845936014762\n",
            "score:  0.9701236543143414\n",
            "epoch: 153 => train loss= 21.653768493882026, test loss= 20.1453435618382\n",
            "score:  0.9701259280059834\n",
            "epoch: 154 => train loss= 21.52106895332787, test loss= 20.021460109756838\n",
            "score:  0.9701272394143322\n",
            "epoch: 155 => train loss= 21.38881699482982, test loss= 19.8991646415148\n",
            "score:  0.9701273512326817\n",
            "epoch: 156 => train loss= 21.258591239226202, test loss= 19.778427054927608\n",
            "score:  0.9701241807568888\n",
            "epoch: 157 => train loss= 21.130434371962146, test loss= 19.65921842599217\n",
            "score:  0.970128120657026\n",
            "epoch: 158 => train loss= 21.003414682260743, test loss= 19.54150849393329\n",
            "score:  0.970125586752335\n",
            "epoch: 159 => train loss= 20.878696219210646, test loss= 19.42527043633163\n",
            "score:  0.9701273461130373\n",
            "epoch: 160 => train loss= 20.754894569859953, test loss= 19.310475984715527\n",
            "score:  0.9701300783810987\n",
            "epoch: 161 => train loss= 20.632094426721178, test loss= 19.19709821578897\n",
            "score:  0.9701315582030132\n",
            "epoch: 162 => train loss= 20.51079379953281, test loss= 19.08511129844408\n",
            "score:  0.970132787857036\n",
            "epoch: 163 => train loss= 20.391552687937374, test loss= 18.974489838611788\n",
            "score:  0.9701367016787905\n",
            "epoch: 164 => train loss= 20.2739874474414, test loss= 18.86520849574696\n",
            "score:  0.9701398196845088\n",
            "epoch: 165 => train loss= 20.157934096383762, test loss= 18.75724320239331\n",
            "score:  0.9701400271060046\n",
            "epoch: 166 => train loss= 20.04276551756214, test loss= 18.650570867304317\n",
            "score:  0.9701378604513439\n",
            "epoch: 167 => train loss= 19.92938786463774, test loss= 18.545168848264787\n",
            "score:  0.970139338666302\n",
            "epoch: 168 => train loss= 19.816653732415084, test loss= 18.441013914240887\n",
            "score:  0.9701402464311338\n",
            "epoch: 169 => train loss= 19.70597172256778, test loss= 18.338084163735896\n",
            "score:  0.9701413112678684\n",
            "epoch: 170 => train loss= 19.59590280444401, test loss= 18.236358073371196\n",
            "score:  0.97013970436004\n",
            "epoch: 171 => train loss= 19.48722315007864, test loss= 18.135815139426743\n",
            "score:  0.9701383877441546\n",
            "epoch: 172 => train loss= 19.38041862060372, test loss= 18.03643479202524\n",
            "score:  0.9701393157575304\n",
            "epoch: 173 => train loss= 19.274091904459915, test loss= 17.938196578930164\n",
            "score:  0.9701351768198618\n",
            "epoch: 174 => train loss= 19.16978777827049, test loss= 17.84108183520181\n",
            "score:  0.9701361225012832\n",
            "epoch: 175 => train loss= 19.065884781759475, test loss= 17.74507049843669\n",
            "score:  0.9701359423907064\n",
            "epoch: 176 => train loss= 18.96297759708856, test loss= 17.650144067524515\n",
            "score:  0.9701350747986071\n",
            "epoch: 177 => train loss= 18.861769356670674, test loss= 17.556284379758193\n",
            "score:  0.9701398290467592\n",
            "epoch: 178 => train loss= 18.76139485542287, test loss= 17.463472564793165\n",
            "score:  0.9701409771425805\n",
            "epoch: 179 => train loss= 18.663388057325093, test loss= 17.371691790885397\n",
            "score:  0.9701442312416452\n",
            "epoch: 180 => train loss= 18.56584633611607, test loss= 17.28092460144949\n",
            "score:  0.970141490281718\n",
            "epoch: 181 => train loss= 18.469621449299567, test loss= 17.19115532918291\n",
            "score:  0.9701383914006925\n",
            "epoch: 182 => train loss= 18.3737259820441, test loss= 17.102367676672387\n",
            "score:  0.9701427654973289\n",
            "epoch: 183 => train loss= 18.278800906659747, test loss= 17.014544356452383\n",
            "score:  0.9701398724816622\n",
            "epoch: 184 => train loss= 18.18508220327637, test loss= 16.927670971445135\n",
            "score:  0.9701417827211282\n",
            "epoch: 185 => train loss= 18.092212405748175, test loss= 16.84173138455678\n",
            "score:  0.9701440879502142\n",
            "epoch: 186 => train loss= 18.000047353374494, test loss= 16.756710548451878\n",
            "score:  0.9701473657063757\n",
            "epoch: 187 => train loss= 17.909161855939402, test loss= 16.672593638617943\n",
            "score:  0.9701448201787793\n",
            "epoch: 188 => train loss= 17.81940809134408, test loss= 16.589367280246087\n",
            "score:  0.9701496510581132\n",
            "epoch: 189 => train loss= 17.731102417162933, test loss= 16.507016185710306\n",
            "score:  0.9701485732222194\n",
            "epoch: 190 => train loss= 17.64367732761195, test loss= 16.42552758449035\n",
            "score:  0.9701493088699734\n",
            "epoch: 191 => train loss= 17.55689115752466, test loss= 16.344887702104945\n",
            "score:  0.9701509401607978\n",
            "epoch: 192 => train loss= 17.470438031919528, test loss= 16.265083199029142\n",
            "score:  0.9701478688831717\n",
            "epoch: 193 => train loss= 17.385811108826978, test loss= 16.186101922669362\n",
            "score:  0.9701497189845439\n",
            "epoch: 194 => train loss= 17.30125921577106, test loss= 16.107930411130955\n",
            "score:  0.9701515632535462\n",
            "epoch: 195 => train loss= 17.217344571344835, test loss= 16.030556270966724\n",
            "score:  0.9701531366494777\n",
            "epoch: 196 => train loss= 17.13439243086941, test loss= 15.953967403034268\n",
            "score:  0.9701535582348305\n",
            "epoch: 197 => train loss= 17.05232084289322, test loss= 15.878152092598905\n",
            "score:  0.9701565246135296\n",
            "epoch: 198 => train loss= 16.971332231053797, test loss= 15.803098274235749\n",
            "score:  0.9701590002995357\n",
            "epoch: 199 => train loss= 16.89120406690985, test loss= 15.728794603347778\n",
            "score:  0.9701590687842897\n",
            "epoch: 200 => train loss= 16.811687276831695, test loss= 15.655230261793184\n",
            "score:  0.970160039139185\n",
            "epoch: 201 => train loss= 16.73294564922995, test loss= 15.582394128388698\n",
            "score:  0.9701602481193491\n",
            "epoch: 202 => train loss= 16.655327695317666, test loss= 15.510275559765953\n",
            "score:  0.9701635139765155\n",
            "epoch: 203 => train loss= 16.578016453195644, test loss= 15.438863530170684\n",
            "score:  0.9701609018449516\n",
            "epoch: 204 => train loss= 16.50216799286496, test loss= 15.36814860599797\n",
            "score:  0.9701610106709191\n",
            "epoch: 205 => train loss= 16.426319350249518, test loss= 15.298120217705236\n",
            "score:  0.9701644071361395\n",
            "epoch: 206 => train loss= 16.351229734029786, test loss= 15.22876791389668\n",
            "score:  0.9701672185971263\n",
            "epoch: 207 => train loss= 16.276961284408163, test loss= 15.160082032474188\n",
            "score:  0.9701682105940874\n",
            "epoch: 208 => train loss= 16.203330504852495, test loss= 15.092053282203857\n",
            "score:  0.970169440828602\n",
            "epoch: 209 => train loss= 16.13056099637329, test loss= 15.024672239735013\n",
            "score:  0.9701705603010279\n",
            "epoch: 210 => train loss= 16.05837789026058, test loss= 14.957929712618697\n",
            "score:  0.9701722810989788\n",
            "epoch: 211 => train loss= 15.987513422117521, test loss= 14.89181657585333\n",
            "score:  0.9701729014000054\n",
            "epoch: 212 => train loss= 15.916680233834136, test loss= 14.826324127649478\n",
            "score:  0.9701729216709079\n",
            "epoch: 213 => train loss= 15.84648935748973, test loss= 14.76144375533701\n",
            "score:  0.9701738376496609\n",
            "epoch: 214 => train loss= 15.777108832050201, test loss= 14.697166786914648\n",
            "score:  0.9701736674532242\n",
            "epoch: 215 => train loss= 15.708079256940259, test loss= 14.633485000442576\n",
            "score:  0.9701733573818577\n",
            "epoch: 216 => train loss= 15.639771872573458, test loss= 14.570390187924907\n",
            "score:  0.9701742031732028\n",
            "epoch: 217 => train loss= 15.572201553575919, test loss= 14.50787410440795\n",
            "score:  0.970175904212318\n",
            "epoch: 218 => train loss= 15.505316189371793, test loss= 14.44592869880537\n",
            "score:  0.9701761714733493\n",
            "epoch: 219 => train loss= 15.439934338043843, test loss= 14.384546394781633\n",
            "score:  0.97017555719532\n",
            "epoch: 220 => train loss= 15.374657478051065, test loss= 14.323719674645506\n",
            "score:  0.9701762942578205\n",
            "epoch: 221 => train loss= 15.309402135036589, test loss= 14.263440837999722\n",
            "score:  0.970170785506852\n",
            "epoch: 222 => train loss= 15.24637288454154, test loss= 14.203703398929049\n",
            "score:  0.9701712967900189\n",
            "epoch: 223 => train loss= 15.182130933458899, test loss= 14.1444992577391\n",
            "score:  0.9701745771573124\n",
            "epoch: 224 => train loss= 15.118593861266735, test loss= 14.085820915169187\n",
            "score:  0.9701761223314493\n",
            "epoch: 225 => train loss= 15.055520429957644, test loss= 14.027661633966243\n",
            "score:  0.9701792546392917\n",
            "epoch: 226 => train loss= 14.993560600118528, test loss= 13.970014333724976\n",
            "score:  0.9701794342354124\n",
            "epoch: 227 => train loss= 14.932659574931389, test loss= 13.91287268672073\n",
            "score:  0.9701804599519451\n",
            "epoch: 228 => train loss= 14.871218266422236, test loss= 13.856229951808546\n",
            "score:  0.9701801201183584\n",
            "epoch: 229 => train loss= 14.81045801220751, test loss= 13.800079809064451\n",
            "score:  0.9701788156664842\n",
            "epoch: 230 => train loss= 14.750289577311142, test loss= 13.744415993040258\n",
            "score:  0.9701783902955482\n",
            "epoch: 231 => train loss= 14.691221296232138, test loss= 13.68923209556218\n",
            "score:  0.9701767978857295\n",
            "epoch: 232 => train loss= 14.632197925109464, test loss= 13.634522095258655\n",
            "score:  0.9701764514673983\n",
            "epoch: 233 => train loss= 14.573417492943188, test loss= 13.580279748663942\n",
            "score:  0.9701796194752191\n",
            "epoch: 234 => train loss= 14.515121566157754, test loss= 13.526498613205362\n",
            "score:  0.9701791173448236\n",
            "epoch: 235 => train loss= 14.457283323071445, test loss= 13.473173317515243\n",
            "score:  0.9701736495367036\n",
            "epoch: 236 => train loss= 14.400622881895055, test loss= 13.42029875265395\n",
            "score:  0.9701673124649985\n",
            "epoch: 237 => train loss= 14.344592369167136, test loss= 13.36786935284358\n",
            "score:  0.9701670134522016\n",
            "epoch: 238 => train loss= 14.288270275155833, test loss= 13.315878732184485\n",
            "score:  0.9701696289802295\n",
            "epoch: 239 => train loss= 14.23244565559906, test loss= 13.26432102272908\n",
            "score:  0.9701724320089274\n",
            "epoch: 240 => train loss= 14.176917416686727, test loss= 13.213190810808998\n",
            "score:  0.9701759697858073\n",
            "epoch: 241 => train loss= 14.121933038758803, test loss= 13.162482700810944\n",
            "score:  0.970173049529076\n",
            "epoch: 242 => train loss= 14.067605511654937, test loss= 13.112192320970841\n",
            "score:  0.9701815673093286\n",
            "epoch: 243 => train loss= 14.014257453115922, test loss= 13.06231305516157\n",
            "score:  0.9701828115618881\n",
            "epoch: 244 => train loss= 13.96066017828165, test loss= 13.01284080646476\n",
            "score:  0.9701836874167862\n",
            "epoch: 245 => train loss= 13.90765357659167, test loss= 12.963770658746968\n",
            "score:  0.970179939737051\n",
            "epoch: 246 => train loss= 13.855428374950392, test loss= 12.915098319410795\n",
            "score:  0.9701824506635747\n",
            "epoch: 247 => train loss= 13.803215552838132, test loss= 12.866818179286295\n",
            "score:  0.9701853260398976\n",
            "epoch: 248 => train loss= 13.75126079942687, test loss= 12.818925466642801\n",
            "score:  0.9701780041636067\n",
            "epoch: 249 => train loss= 13.700987018231835, test loss= 12.771416820526124\n",
            "score:  0.9701784866603052\n",
            "epoch: 250 => train loss= 13.650291943645945, test loss= 12.724286668804062\n",
            "score:  0.970184564301259\n",
            "epoch: 251 => train loss= 13.599968426068928, test loss= 12.677529804290288\n",
            "score:  0.9701871192306557\n",
            "epoch: 252 => train loss= 13.549795439713995, test loss= 12.631142240267968\n",
            "score:  0.9701865087301049\n",
            "epoch: 253 => train loss= 13.499814426029861, test loss= 12.585120008451732\n",
            "score:  0.9701895385459259\n",
            "epoch: 254 => train loss= 13.450514274146878, test loss= 12.539458360157761\n",
            "score:  0.9701952836592365\n",
            "epoch: 255 => train loss= 13.401891585235717, test loss= 12.494152734987438\n",
            "score:  0.9701968086021748\n",
            "epoch: 256 => train loss= 13.353305725367319, test loss= 12.44919949538049\n",
            "score:  0.9701982554263764\n",
            "epoch: 257 => train loss= 13.305773284443028, test loss= 12.404594553302426\n",
            "score:  0.9701984963108901\n",
            "epoch: 258 => train loss= 13.25788196031635, test loss= 12.360334021474404\n",
            "score:  0.970196291809808\n",
            "epoch: 259 => train loss= 13.210333594839488, test loss= 12.316414223038233\n",
            "score:  0.9701978855340251\n",
            "epoch: 260 => train loss= 13.164036630731857, test loss= 12.27283078203713\n",
            "score:  0.9701956859103773\n",
            "epoch: 261 => train loss= 13.117663869936925, test loss= 12.229580304094853\n",
            "score:  0.9701975387994443\n",
            "epoch: 262 => train loss= 13.071178772761439, test loss= 12.186658504797931\n",
            "score:  0.9701983095828164\n",
            "epoch: 263 => train loss= 13.0249222209919, test loss= 12.144061778305154\n",
            "score:  0.9702034393039586\n",
            "epoch: 264 => train loss= 12.9794109098794, test loss= 12.101785925199401\n",
            "score:  0.970202432784337\n",
            "epoch: 265 => train loss= 12.933901122634156, test loss= 12.059828055308277\n",
            "score:  0.9702014402299831\n",
            "epoch: 266 => train loss= 12.889132992250355, test loss= 12.01818459400077\n",
            "score:  0.9702026798550424\n",
            "epoch: 267 => train loss= 12.844878667322144, test loss= 11.97685175867223\n",
            "score:  0.9701989802137743\n",
            "epoch: 268 => train loss= 12.800743712491167, test loss= 11.935826664963619\n",
            "score:  0.9702006858374821\n",
            "epoch: 269 => train loss= 12.756574113939017, test loss= 11.895105261272855\n",
            "score:  0.9702046701018365\n",
            "epoch: 270 => train loss= 12.713103466081538, test loss= 11.85468392024621\n",
            "score:  0.9702044514977057\n",
            "epoch: 271 => train loss= 12.669704880843236, test loss= 11.814559820382033\n",
            "score:  0.9702085789175596\n",
            "epoch: 272 => train loss= 12.627073535002662, test loss= 11.77472919247526\n",
            "score:  0.9702089981174994\n",
            "epoch: 273 => train loss= 12.584311936812027, test loss= 11.73518925078594\n",
            "score:  0.9702086411402299\n",
            "epoch: 274 => train loss= 12.541902506734644, test loss= 11.69593691327355\n",
            "score:  0.9702074808068861\n",
            "epoch: 275 => train loss= 12.499800545655145, test loss= 11.656969145804212\n",
            "score:  0.9702082564998011\n",
            "epoch: 276 => train loss= 12.458396015380417, test loss= 11.618282645618011\n",
            "score:  0.9702108041741692\n",
            "epoch: 277 => train loss= 12.416986447424922, test loss= 11.579874176130021\n",
            "score:  0.9702101544257042\n",
            "epoch: 278 => train loss= 12.37629327629309, test loss= 11.541741109877076\n",
            "score:  0.9702113897239651\n",
            "epoch: 279 => train loss= 12.335129508250677, test loss= 11.503880283236503\n",
            "score:  0.9702133226401609\n",
            "epoch: 280 => train loss= 12.29449640701174, test loss= 11.466288711464703\n",
            "score:  0.9702148177140357\n",
            "epoch: 281 => train loss= 12.254172230288463, test loss= 11.42896357918462\n",
            "score:  0.9702182203609235\n",
            "epoch: 282 => train loss= 12.214176073884765, test loss= 11.391901849015442\n",
            "score:  0.9702202808060439\n",
            "epoch: 283 => train loss= 12.174603745682349, test loss= 11.355100887761989\n",
            "score:  0.9702207947105743\n",
            "epoch: 284 => train loss= 12.134873784611697, test loss= 11.318558121982374\n",
            "score:  0.9702190105222164\n",
            "epoch: 285 => train loss= 12.095737642795592, test loss= 11.282271096964816\n",
            "score:  0.9702209798786274\n",
            "epoch: 286 => train loss= 12.056875774914083, test loss= 11.246236726589734\n",
            "score:  0.9702216581998716\n",
            "epoch: 287 => train loss= 12.018881798270971, test loss= 11.210452520598968\n",
            "score:  0.9702227698443758\n",
            "epoch: 288 => train loss= 11.980688174975667, test loss= 11.174915834694172\n",
            "score:  0.9702230404255656\n",
            "epoch: 289 => train loss= 11.942512104113407, test loss= 11.139624199990568\n",
            "score:  0.9702220642059527\n",
            "epoch: 290 => train loss= 11.90476630922182, test loss= 11.104575225372905\n",
            "score:  0.9702251215403206\n",
            "epoch: 291 => train loss= 11.867167075647592, test loss= 11.069765981541922\n",
            "score:  0.9702268145074412\n",
            "epoch: 292 => train loss= 11.829782083063867, test loss= 11.035194160995223\n",
            "score:  0.9702275711598204\n",
            "epoch: 293 => train loss= 11.792442002539152, test loss= 11.000857441603731\n",
            "score:  0.9702281500065728\n",
            "epoch: 294 => train loss= 11.755329692473785, test loss= 10.966753451500908\n",
            "score:  0.9702287199841819\n",
            "epoch: 295 => train loss= 11.719361046244638, test loss= 10.932879832950798\n",
            "score:  0.970229786320105\n",
            "epoch: 296 => train loss= 11.683292241748234, test loss= 10.899234206186803\n",
            "score:  0.9702304727028384\n",
            "epoch: 297 => train loss= 11.647036600341862, test loss= 10.865814316392745\n",
            "score:  0.9702310606596233\n",
            "epoch: 298 => train loss= 11.611007714986702, test loss= 10.832617908815875\n",
            "score:  0.9702315680767887\n",
            "epoch: 299 => train loss= 11.575141757316887, test loss= 10.799642757177352\n",
            "score:  0.9702321628450221\n",
            "epoch: 300 => train loss= 11.539588313394168, test loss= 10.766886647159476\n",
            "score:  0.9702325558374586\n",
            "epoch: 301 => train loss= 11.5043661292156, test loss= 10.734347423970304\n",
            "score:  0.9702323926631005\n",
            "epoch: 302 => train loss= 11.469435972083872, test loss= 10.702022998049708\n",
            "score:  0.9702322959904432\n",
            "epoch: 303 => train loss= 11.434654500799365, test loss= 10.669911243021488\n",
            "score:  0.9702338093981103\n",
            "epoch: 304 => train loss= 11.400402211125687, test loss= 10.638009900147798\n",
            "score:  0.9702348198873043\n",
            "epoch: 305 => train loss= 11.366166453639847, test loss= 10.606316958377564\n",
            "score:  0.970235787609221\n",
            "epoch: 306 => train loss= 11.332345894281742, test loss= 10.574830385877565\n",
            "score:  0.9702365130291993\n",
            "epoch: 307 => train loss= 11.298668457774, test loss= 10.54354819732827\n",
            "score:  0.9702371132295173\n",
            "epoch: 308 => train loss= 11.264876146692743, test loss= 10.51246842095767\n",
            "score:  0.9702376934917905\n",
            "epoch: 309 => train loss= 11.231269086524843, test loss= 10.481589100245506\n",
            "score:  0.9702389974756691\n",
            "epoch: 310 => train loss= 11.198307718458762, test loss= 10.450908227940465\n",
            "score:  0.9702394413405113\n",
            "epoch: 311 => train loss= 11.165784168911708, test loss= 10.420423982999264\n",
            "score:  0.9702403134863493\n",
            "epoch: 312 => train loss= 11.133279082362359, test loss= 10.390134437586934\n",
            "score:  0.9702406909616936\n",
            "epoch: 313 => train loss= 11.100628069812963, test loss= 10.360037781250705\n",
            "score:  0.9702414480305299\n",
            "epoch: 314 => train loss= 11.068680117559744, test loss= 10.330132138918316\n",
            "score:  0.9702419934805914\n",
            "epoch: 315 => train loss= 11.037062804282021, test loss= 10.30041571829138\n",
            "score:  0.9702428286770597\n",
            "epoch: 316 => train loss= 11.00533345390536, test loss= 10.270886699672001\n",
            "score:  0.9702435281598863\n",
            "epoch: 317 => train loss= 10.97399942173061, test loss= 10.241543328799542\n",
            "score:  0.970243619618218\n",
            "epoch: 318 => train loss= 10.942570921173758, test loss= 10.212383919748767\n",
            "score:  0.9702444018788257\n",
            "epoch: 319 => train loss= 10.911292024374207, test loss= 10.18340667989105\n",
            "score:  0.9702452107286987\n",
            "epoch: 320 => train loss= 10.880201042393114, test loss= 10.154609903925305\n",
            "score:  0.9702459070584799\n",
            "epoch: 321 => train loss= 10.849210999904411, test loss= 10.125991921617377\n",
            "score:  0.9702463861366853\n",
            "epoch: 322 => train loss= 10.818687407994352, test loss= 10.097551093751063\n",
            "score:  0.9702472255336699\n",
            "epoch: 323 => train loss= 10.78808782891431, test loss= 10.069285744686185\n",
            "score:  0.9702477281812902\n",
            "epoch: 324 => train loss= 10.757553467080816, test loss= 10.04119428726343\n",
            "score:  0.9702479410232998\n",
            "epoch: 325 => train loss= 10.727758928387377, test loss= 10.013275149592593\n",
            "score:  0.9702486204285207\n",
            "epoch: 326 => train loss= 10.697721654029262, test loss= 9.985526705553772\n",
            "score:  0.9702496297558197\n",
            "epoch: 327 => train loss= 10.668047188971036, test loss= 9.957947362123466\n",
            "score:  0.9702500063531925\n",
            "epoch: 328 => train loss= 10.63834685339385, test loss= 9.930535638042496\n",
            "score:  0.9702512692940729\n",
            "epoch: 329 => train loss= 10.60926285176921, test loss= 9.903289924787753\n",
            "score:  0.9702519575332319\n",
            "epoch: 330 => train loss= 10.58026680706433, test loss= 9.876208772652099\n",
            "score:  0.9702525544267484\n",
            "epoch: 331 => train loss= 10.551109541892018, test loss= 9.849290703075479\n",
            "score:  0.9702526612549067\n",
            "epoch: 332 => train loss= 10.522043565715691, test loss= 9.8225342935628\n",
            "score:  0.9702540479042226\n",
            "epoch: 333 => train loss= 10.493393253970021, test loss= 9.795937970905246\n",
            "score:  0.9702545779241684\n",
            "epoch: 334 => train loss= 10.464853456900762, test loss= 9.769500382266827\n",
            "score:  0.9702549916330465\n",
            "epoch: 335 => train loss= 10.43625168779035, test loss= 9.743220121377991\n",
            "score:  0.9702550421376605\n",
            "epoch: 336 => train loss= 10.408283294414238, test loss= 9.7170958216537\n",
            "score:  0.9702550265851533\n",
            "epoch: 337 => train loss= 10.380509236248074, test loss= 9.691126105171689\n",
            "score:  0.9702561631993599\n",
            "epoch: 338 => train loss= 10.352722490235402, test loss= 9.665309496333817\n",
            "score:  0.9702554020406713\n",
            "epoch: 339 => train loss= 10.324922694132797, test loss= 9.639644820725216\n",
            "score:  0.9702551911819279\n",
            "epoch: 340 => train loss= 10.29760588317092, test loss= 9.614130690650157\n",
            "score:  0.9702549699497469\n",
            "epoch: 341 => train loss= 10.270131393114017, test loss= 9.588765786405196\n",
            "score:  0.9702559897478142\n",
            "epoch: 342 => train loss= 10.243159104818195, test loss= 9.56354868846454\n",
            "score:  0.9702568362619528\n",
            "epoch: 343 => train loss= 10.215952045687992, test loss= 9.53847812393377\n",
            "score:  0.970258161799784\n",
            "epoch: 344 => train loss= 10.188971216108179, test loss= 9.513552774726481\n",
            "score:  0.9702588772676178\n",
            "epoch: 345 => train loss= 10.16237264122694, test loss= 9.48877143722049\n",
            "score:  0.9702604501315188\n",
            "epoch: 346 => train loss= 10.135866934738013, test loss= 9.46413278854546\n",
            "score:  0.9702612163964979\n",
            "epoch: 347 => train loss= 10.109218979653503, test loss= 9.439635671760843\n",
            "score:  0.9702619962496476\n",
            "epoch: 348 => train loss= 10.083077842173727, test loss= 9.415278869064625\n",
            "score:  0.9702619990189253\n",
            "epoch: 349 => train loss= 10.056831375723895, test loss= 9.391061247927802\n",
            "score:  0.9702625099310584\n",
            "epoch: 350 => train loss= 10.030699936375113, test loss= 9.366981572914668\n",
            "score:  0.9702627368623039\n",
            "epoch: 351 => train loss= 10.004734797256564, test loss= 9.343038693917068\n",
            "score:  0.97026329365432\n",
            "epoch: 352 => train loss= 9.978815433316353, test loss= 9.319231418813592\n",
            "score:  0.9702650797558686\n",
            "epoch: 353 => train loss= 9.953275919217644, test loss= 9.295558488638388\n",
            "score:  0.9702655364260911\n",
            "epoch: 354 => train loss= 9.9279321906957, test loss= 9.27201888645199\n",
            "score:  0.9702662607936081\n",
            "epoch: 355 => train loss= 9.902909713937792, test loss= 9.248611464929045\n",
            "score:  0.9702668679684572\n",
            "epoch: 356 => train loss= 9.877637344860837, test loss= 9.225335123826142\n",
            "score:  0.9702668993101442\n",
            "epoch: 357 => train loss= 9.853344194423066, test loss= 9.202188815318006\n",
            "score:  0.9702677108289045\n",
            "epoch: 358 => train loss= 9.828619003917026, test loss= 9.179171384211038\n",
            "score:  0.970269070724497\n",
            "epoch: 359 => train loss= 9.803739346958519, test loss= 9.156281708512042\n",
            "score:  0.9702698764935137\n",
            "epoch: 360 => train loss= 9.779151832278734, test loss= 9.133518774918903\n",
            "score:  0.9702692888124937\n",
            "epoch: 361 => train loss= 9.754702741580102, test loss= 9.110881654597119\n",
            "score:  0.9702692733301691\n",
            "epoch: 362 => train loss= 9.730303616766935, test loss= 9.088369258179153\n",
            "score:  0.970269010370122\n",
            "epoch: 363 => train loss= 9.706207660427449, test loss= 9.065980579008112\n",
            "score:  0.9702702259839222\n",
            "epoch: 364 => train loss= 9.682090440169414, test loss= 9.043714472365705\n",
            "score:  0.970271344204429\n",
            "epoch: 365 => train loss= 9.657948321830787, test loss= 9.02156994186464\n",
            "score:  0.9702723757856762\n",
            "epoch: 366 => train loss= 9.63417537898295, test loss= 8.999546001327785\n",
            "score:  0.9702751057498601\n",
            "epoch: 367 => train loss= 9.61098623950098, test loss= 8.977641521750584\n",
            "score:  0.9702756469495971\n",
            "epoch: 368 => train loss= 9.58776310764533, test loss= 8.95585571944229\n",
            "score:  0.97027650440414\n",
            "epoch: 369 => train loss= 9.564430241923937, test loss= 8.93418760492995\n",
            "score:  0.9702771417246979\n",
            "epoch: 370 => train loss= 9.541028591051473, test loss= 8.912636245518039\n",
            "score:  0.9702775747797571\n",
            "epoch: 371 => train loss= 9.517712224015934, test loss= 8.891200716777515\n",
            "score:  0.9702782438290232\n",
            "epoch: 372 => train loss= 9.494484140114999, test loss= 8.869880067279448\n",
            "score:  0.9702788712062703\n",
            "epoch: 373 => train loss= 9.4714475722928, test loss= 8.848673378879374\n",
            "score:  0.9702796476612023\n",
            "epoch: 374 => train loss= 9.448492509153628, test loss= 8.827579727490743\n",
            "score:  0.9702805467068842\n",
            "epoch: 375 => train loss= 9.425809164057375, test loss= 8.806598200918511\n",
            "score:  0.9702805549422722\n",
            "epoch: 376 => train loss= 9.40326992846537, test loss= 8.785727981389043\n",
            "score:  0.9702811818685595\n",
            "epoch: 377 => train loss= 9.380684415582435, test loss= 8.76496813395036\n",
            "score:  0.9702823761558029\n",
            "epoch: 378 => train loss= 9.358498518847707, test loss= 8.744317737607025\n",
            "score:  0.9702829901267833\n",
            "epoch: 379 => train loss= 9.336266810983364, test loss= 8.723775976582578\n",
            "score:  0.970283702643811\n",
            "epoch: 380 => train loss= 9.314008361737816, test loss= 8.703341987345787\n",
            "score:  0.970284498108324\n",
            "epoch: 381 => train loss= 9.291925366283879, test loss= 8.683014916499872\n",
            "score:  0.9702848424695206\n",
            "epoch: 382 => train loss= 9.270273862561542, test loss= 8.66279396391413\n",
            "score:  0.9702856536335643\n",
            "epoch: 383 => train loss= 9.248443426535232, test loss= 8.642678262044987\n",
            "score:  0.9702858953121821\n",
            "epoch: 384 => train loss= 9.226743371902183, test loss= 8.622667037511802\n",
            "score:  0.970286850529035\n",
            "epoch: 385 => train loss= 9.205167300655864, test loss= 8.602759419945238\n",
            "score:  0.970287468438978\n",
            "epoch: 386 => train loss= 9.183785315019954, test loss= 8.582954633605572\n",
            "score:  0.970288447001837\n",
            "epoch: 387 => train loss= 9.162542404613797, test loss= 8.563251854218159\n",
            "score:  0.9702879824137679\n",
            "epoch: 388 => train loss= 9.141570940995441, test loss= 8.543650412161124\n",
            "score:  0.9702890763537472\n",
            "epoch: 389 => train loss= 9.12033896335288, test loss= 8.52414940167696\n",
            "score:  0.970289787212879\n",
            "epoch: 390 => train loss= 9.099725295977029, test loss= 8.504748083289018\n",
            "score:  0.9702901104055452\n",
            "epoch: 391 => train loss= 9.078881572881773, test loss= 8.485445725218375\n",
            "score:  0.9702910035764819\n",
            "epoch: 392 => train loss= 9.057989571584773, test loss= 8.466241526088035\n",
            "score:  0.970291004101465\n",
            "epoch: 393 => train loss= 9.03767103864862, test loss= 8.447134810202012\n",
            "score:  0.970292123651767\n",
            "epoch: 394 => train loss= 9.017319749119215, test loss= 8.42812474769882\n",
            "score:  0.9702927301197427\n",
            "epoch: 395 => train loss= 8.996932785634172, test loss= 8.409210647296424\n",
            "score:  0.9702933053749201\n",
            "epoch: 396 => train loss= 8.976996831468549, test loss= 8.390391786242612\n",
            "score:  0.9702936991041268\n",
            "epoch: 397 => train loss= 8.95673763713875, test loss= 8.371667461029848\n",
            "score:  0.9702942853198756\n",
            "epoch: 398 => train loss= 8.936466665541692, test loss= 8.353036945774441\n",
            "score:  0.9702947830103397\n",
            "epoch: 399 => train loss= 8.91638166415239, test loss= 8.334499543756246\n",
            "score:  0.9702954606633707\n",
            "epoch: 400 => train loss= 8.896298957301823, test loss= 8.316054544246702\n",
            "score:  0.9702958629828431\n",
            "epoch: 401 => train loss= 8.87652855596155, test loss= 8.297701279322306\n",
            "score:  0.970295980023384\n",
            "epoch: 402 => train loss= 8.856893408232729, test loss= 8.279439088428582\n",
            "score:  0.9702955662279119\n",
            "epoch: 403 => train loss= 8.837196385289003, test loss= 8.26126733673091\n",
            "score:  0.970296821184067\n",
            "epoch: 404 => train loss= 8.817702229621917, test loss= 8.243185224209302\n",
            "score:  0.9702975071339133\n",
            "epoch: 405 => train loss= 8.798158389996583, test loss= 8.225192132841777\n",
            "score:  0.9702981494242281\n",
            "epoch: 406 => train loss= 8.77874939546261, test loss= 8.207287409821072\n",
            "score:  0.9702995310106981\n",
            "epoch: 407 => train loss= 8.759356083566225, test loss= 8.189470347966633\n",
            "score:  0.970299648508862\n",
            "epoch: 408 => train loss= 8.740083099888404, test loss= 8.171740402073615\n",
            "score:  0.9703006543627057\n",
            "epoch: 409 => train loss= 8.721121162486357, test loss= 8.1540968662355\n",
            "score:  0.9703014791402482\n",
            "epoch: 410 => train loss= 8.702392620221946, test loss= 8.136539123644214\n",
            "score:  0.9703016750424218\n",
            "epoch: 411 => train loss= 8.68363722191475, test loss= 8.119066597911917\n",
            "score:  0.9703028349549291\n",
            "epoch: 412 => train loss= 8.664979886714612, test loss= 8.10167859613751\n",
            "score:  0.9703024405432895\n",
            "epoch: 413 => train loss= 8.646182885552898, test loss= 8.084374624461944\n",
            "score:  0.9703003601508332\n",
            "epoch: 414 => train loss= 8.627732620919172, test loss= 8.067154203696424\n",
            "score:  0.9702991692608991\n",
            "epoch: 415 => train loss= 8.609258765752612, test loss= 8.050016663825282\n",
            "score:  0.9703000070724296\n",
            "epoch: 416 => train loss= 8.590852879984519, test loss= 8.032961254926036\n",
            "score:  0.9702978973423546\n",
            "epoch: 417 => train loss= 8.572673971337595, test loss= 8.015987610275095\n",
            "score:  0.9703002448828411\n",
            "epoch: 418 => train loss= 8.554497940797948, test loss= 7.999094808443065\n",
            "score:  0.970302198761828\n",
            "epoch: 419 => train loss= 8.536220795218991, test loss= 7.9822823016416455\n",
            "score:  0.970303502045827\n",
            "epoch: 420 => train loss= 8.51799381252923, test loss= 7.965549566541885\n",
            "score:  0.9703005730417447\n",
            "epoch: 421 => train loss= 8.500143322318076, test loss= 7.948896352713707\n",
            "score:  0.970295542408901\n",
            "epoch: 422 => train loss= 8.482753497814809, test loss= 7.932322253008542\n",
            "score:  0.9703034802461358\n",
            "epoch: 423 => train loss= 8.465240004845215, test loss= 7.915825741752139\n",
            "score:  0.9703050924115334\n",
            "epoch: 424 => train loss= 8.447595858307196, test loss= 7.899406741366667\n",
            "score:  0.9703023629789695\n",
            "epoch: 425 => train loss= 8.430055025691388, test loss= 7.883065027810039\n",
            "score:  0.9703060849997243\n",
            "epoch: 426 => train loss= 8.412451570364405, test loss= 7.866799580967119\n",
            "score:  0.9703041198562415\n",
            "epoch: 427 => train loss= 8.395085920860744, test loss= 7.850610285858128\n",
            "score:  0.9703016507817341\n",
            "epoch: 428 => train loss= 8.37773086050109, test loss= 7.834496647050053\n",
            "score:  0.9703036673435422\n",
            "epoch: 429 => train loss= 8.360307857140503, test loss= 7.818457807357921\n",
            "score:  0.9703048293312552\n",
            "epoch: 430 => train loss= 8.343240702485886, test loss= 7.802493308647049\n",
            "score:  0.9703028615798744\n",
            "epoch: 431 => train loss= 8.32611230844952, test loss= 7.7866028635590165\n",
            "score:  0.9703056215585778\n",
            "epoch: 432 => train loss= 8.308863674578387, test loss= 7.770785614194275\n",
            "score:  0.9703049025712533\n",
            "epoch: 433 => train loss= 8.291832112279856, test loss= 7.755041307568001\n",
            "score:  0.970302313438859\n",
            "epoch: 434 => train loss= 8.275044176827853, test loss= 7.739369576552819\n",
            "score:  0.9703050508610421\n",
            "epoch: 435 => train loss= 8.258224459600587, test loss= 7.723769535985562\n",
            "score:  0.9703067938652867\n",
            "epoch: 436 => train loss= 8.241515572268995, test loss= 7.708240765592326\n",
            "score:  0.9703093778308931\n",
            "epoch: 437 => train loss= 8.224668342237399, test loss= 7.692782716391838\n",
            "score:  0.9703094796200025\n",
            "epoch: 438 => train loss= 8.208177325325169, test loss= 7.677395083909698\n",
            "score:  0.9703142300508364\n",
            "epoch: 439 => train loss= 8.191767655089256, test loss= 7.662077054245906\n",
            "score:  0.9703151355291548\n",
            "epoch: 440 => train loss= 8.175107082241624, test loss= 7.646828429228594\n",
            "score:  0.970316923206584\n",
            "epoch: 441 => train loss= 8.158742319779265, test loss= 7.631648674809555\n",
            "score:  0.9703159521060534\n",
            "epoch: 442 => train loss= 8.14237907054957, test loss= 7.6165375211707085\n",
            "score:  0.970314015390475\n",
            "epoch: 443 => train loss= 8.12621843945139, test loss= 7.601494573526554\n",
            "score:  0.9703150858490867\n",
            "epoch: 444 => train loss= 8.11004141808202, test loss= 7.586519158690163\n",
            "score:  0.9703164629783756\n",
            "epoch: 445 => train loss= 8.093766625212895, test loss= 7.571610800754864\n",
            "score:  0.9703180777802127\n",
            "epoch: 446 => train loss= 8.077662653498383, test loss= 7.556769032739687\n",
            "score:  0.970319852798101\n",
            "epoch: 447 => train loss= 8.061530841099888, test loss= 7.5419933975541165\n",
            "score:  0.9703208537823915\n",
            "epoch: 448 => train loss= 8.045610636296097, test loss= 7.52728350762535\n",
            "score:  0.970319192497303\n",
            "epoch: 449 => train loss= 8.029729209138997, test loss= 7.512639111677806\n",
            "score:  0.9703187038625937\n",
            "epoch: 450 => train loss= 8.01385660593079, test loss= 7.498059691716721\n",
            "score:  0.9703190387200427\n",
            "epoch: 451 => train loss= 7.998030700753814, test loss= 7.483544759122672\n",
            "score:  0.9703167898266836\n",
            "epoch: 452 => train loss= 7.982533507313414, test loss= 7.469094066825134\n",
            "score:  0.9703168905020679\n",
            "epoch: 453 => train loss= 7.967009066179151, test loss= 7.45470702700678\n",
            "score:  0.9703177084775463\n",
            "epoch: 454 => train loss= 7.95148837996217, test loss= 7.440383170201228\n",
            "score:  0.9703154001451595\n",
            "epoch: 455 => train loss= 7.935992485060495, test loss= 7.426122297189738\n",
            "score:  0.9703222495332889\n",
            "epoch: 456 => train loss= 7.920903860445496, test loss= 7.411923361685396\n",
            "score:  0.9703272361313439\n",
            "epoch: 457 => train loss= 7.905898813433661, test loss= 7.397786086433319\n",
            "score:  0.9703302107677505\n",
            "epoch: 458 => train loss= 7.890612094939321, test loss= 7.3837102068535385\n",
            "score:  0.9703302551003732\n",
            "epoch: 459 => train loss= 7.875315505815728, test loss= 7.369695523640384\n",
            "score:  0.97033019360778\n",
            "epoch: 460 => train loss= 7.860217705478665, test loss= 7.35574164592263\n",
            "score:  0.9703319936098496\n",
            "epoch: 461 => train loss= 7.845154293424041, test loss= 7.341848051522201\n",
            "score:  0.9703311259699064\n",
            "epoch: 462 => train loss= 7.830374228345349, test loss= 7.328014531866778\n",
            "score:  0.9703319356919116\n",
            "epoch: 463 => train loss= 7.815478801905072, test loss= 7.314240584342644\n",
            "score:  0.9703340978716339\n",
            "epoch: 464 => train loss= 7.801057935506106, test loss= 7.300525732835134\n",
            "score:  0.9703341578340734\n",
            "epoch: 465 => train loss= 7.78633151205107, test loss= 7.286869739258238\n",
            "score:  0.9703354878844872\n",
            "epoch: 466 => train loss= 7.771568145210925, test loss= 7.273272139612561\n",
            "score:  0.9703333222629794\n",
            "epoch: 467 => train loss= 7.756908533340764, test loss= 7.259732795449404\n",
            "score:  0.9703319556463414\n",
            "epoch: 468 => train loss= 7.742359599764077, test loss= 7.246251280373856\n",
            "score:  0.9703323454057484\n",
            "epoch: 469 => train loss= 7.727886741133944, test loss= 7.232827107322977\n",
            "score:  0.9703309696047584\n",
            "epoch: 470 => train loss= 7.713384440798659, test loss= 7.219460029384386\n",
            "score:  0.970335067125662\n",
            "epoch: 471 => train loss= 7.699179608414297, test loss= 7.206149317450443\n",
            "score:  0.9703381135954623\n",
            "epoch: 472 => train loss= 7.684851337107232, test loss= 7.192894684209884\n",
            "score:  0.9703373631237875\n",
            "epoch: 473 => train loss= 7.670512469584204, test loss= 7.179696027613893\n",
            "score:  0.9703382260195206\n",
            "epoch: 474 => train loss= 7.65617042013465, test loss= 7.166552886962891\n",
            "score:  0.9703388614659726\n",
            "epoch: 475 => train loss= 7.641908529111702, test loss= 7.153464927518067\n",
            "score:  0.970340084112559\n",
            "epoch: 476 => train loss= 7.627729087843755, test loss= 7.140431763241126\n",
            "score:  0.9703405955846952\n",
            "epoch: 477 => train loss= 7.613904327295556, test loss= 7.127453097239698\n",
            "score:  0.9703421442709783\n",
            "epoch: 478 => train loss= 7.5998747721903275, test loss= 7.1145285197488946\n",
            "score:  0.9703425864216765\n",
            "epoch: 479 => train loss= 7.586125935542042, test loss= 7.101657765607039\n",
            "score:  0.9703433296712245\n",
            "epoch: 480 => train loss= 7.5721277291840865, test loss= 7.088840479290659\n",
            "score:  0.9703439004189822\n",
            "epoch: 481 => train loss= 7.5582359855200725, test loss= 7.076076339389279\n",
            "score:  0.970344356489551\n",
            "epoch: 482 => train loss= 7.544534957141601, test loss= 7.063365023205246\n",
            "score:  0.9703447330471507\n",
            "epoch: 483 => train loss= 7.531087855810636, test loss= 7.050706208614279\n",
            "score:  0.9703442296397994\n",
            "epoch: 484 => train loss= 7.517739697732305, test loss= 7.0380996280109756\n",
            "score:  0.9703453276308183\n",
            "epoch: 485 => train loss= 7.504170179444812, test loss= 7.025544855084439\n",
            "score:  0.97034653081109\n",
            "epoch: 486 => train loss= 7.490752362432797, test loss= 7.0130415637145544\n",
            "score:  0.970345080793188\n",
            "epoch: 487 => train loss= 7.477357398621537, test loss= 7.000589609268259\n",
            "score:  0.9703448047952156\n",
            "epoch: 488 => train loss= 7.464195449632221, test loss= 6.988188600857077\n",
            "score:  0.970346518508707\n",
            "epoch: 489 => train loss= 7.450759809460958, test loss= 6.975838098355702\n",
            "score:  0.9703473169126912\n",
            "epoch: 490 => train loss= 7.437502680829238, test loss= 6.96353785205033\n",
            "score:  0.9703476628109604\n",
            "epoch: 491 => train loss= 7.424298715195754, test loss= 6.951287584455033\n",
            "score:  0.9703478898387836\n",
            "epoch: 492 => train loss= 7.410975833087987, test loss= 6.939086999177449\n",
            "score:  0.9703477243333054\n",
            "epoch: 493 => train loss= 7.397939093853737, test loss= 6.926935819599793\n",
            "score:  0.9703478122795223\n",
            "epoch: 494 => train loss= 7.384832828891871, test loss= 6.914833730037766\n",
            "score:  0.9703482210967012\n",
            "epoch: 495 => train loss= 7.3716834180587085, test loss= 6.902780413267113\n",
            "score:  0.9703494780014629\n",
            "epoch: 496 => train loss= 7.358562492563374, test loss= 6.890775520916676\n",
            "score:  0.9703509029547027\n",
            "epoch: 497 => train loss= 7.345516149443, test loss= 6.878818750620846\n",
            "score:  0.9703515578038316\n",
            "epoch: 498 => train loss= 7.332537495385831, test loss= 6.866909861803532\n",
            "score:  0.9703523957007627\n",
            "epoch: 499 => train loss= 7.319632167619254, test loss= 6.855048555612564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WTFv9nfL6SO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "63cf061d-5c10-45f6-ef15-3c6c16c15c6e"
      },
      "source": [
        "plt.plot(train_losses, color=\"b\", label=\"train loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(test_losses, color=\"g\", label=\"test loss\")\n",
        "plt.show()\n",
        "plt.plot(accs, color=\"b\", label=\"accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxEoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckp14H1TFHcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "c3d7200f-fe3d-46d1-9c76-38f8d4135ad5"
      },
      "source": [
        "pred = model(torch.from_numpy(x_train).float()).detach().numpy()\n",
        "plt.scatter(x_train, y_train)\n",
        "plt.plot(x_train, pred, color=\"r\")\n",
        "plt.show()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j7i4GG9LRYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}